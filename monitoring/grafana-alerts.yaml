# Grafana Alert Rules for GitHub Gists API
# Import these into Grafana Cloud or self-hosted Grafana

apiVersion: 1

groups:
  - name: gist-api-alerts
    interval: 1m
    rules:
      # High Error Rate
      - uid: gist_api_high_error_rate
        title: High Error Rate (5xx)
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                (
                  sum(rate(http_requests_total{job="github-gists-api",status=~"5.."}[5m]))
                  /
                  sum(rate(http_requests_total{job="github-gists-api"}[5m]))
                ) > 0.05
              instant: true
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "Error rate is {{ humanizePercentage $value }} (threshold: 5%)"
          summary: "GitHub Gists API has high error rate"
        labels:
          severity: warning
          team: backend

      # High Latency (p95)
      - uid: gist_api_high_latency
        title: High Response Latency
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                histogram_quantile(0.95, 
                  rate(http_request_duration_seconds_bucket{job="github-gists-api"}[5m])
                ) > 2
              instant: true
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "p95 latency is {{ humanizeDuration $value }} (threshold: 2s)"
          summary: "GitHub Gists API response time is slow"
        labels:
          severity: warning
          team: backend

      # API Down
      - uid: gist_api_down
        title: API Health Check Failed
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                up{job="github-gists-api"} == 0
              instant: true
        noDataState: Alerting
        execErrState: Error
        for: 2m
        annotations:
          description: "GitHub Gists API is down for {{ $value }}s"
          summary: "GitHub Gists API is not responding"
        labels:
          severity: critical
          team: backend
          page: "true"

      # Low Pod Count
      - uid: gist_api_low_pods
        title: Low Pod Count
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                count(up{job="github-gists-api"} == 1) < 2
              instant: true
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "Only {{ $value }} pods running (expected: >= 2)"
          summary: "GitHub Gists API has insufficient replicas"
        labels:
          severity: warning
          team: devops

      # SSL Certificate Expiry
      - uid: gist_api_ssl_expiry
        title: SSL Certificate Expiring Soon
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                (ssl_certificate_expiry_seconds{domain="gists-api.example.com"} - time()) / 86400 < 30
              instant: true
        noDataState: NoData
        execErrState: Error
        for: 1h
        annotations:
          description: "SSL certificate expires in {{ humanizeDuration $value }}"
          summary: "SSL certificate for gists-api.example.com is expiring soon"
        labels:
          severity: warning
          team: devops

      # GitHub API Rate Limit
      - uid: gist_api_github_rate_limit
        title: GitHub API Rate Limit Warning
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                increase(http_requests_total{job="github-gists-api",status="429"}[5m]) > 5
              instant: true
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "{{ $value }} rate limit errors in last 5 minutes"
          summary: "GitHub API rate limit being hit frequently"
        labels:
          severity: warning
          team: backend

      # Memory Usage High
      - uid: gist_api_memory_high
        title: High Memory Usage
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                (
                  container_memory_usage_bytes{pod=~"github-gists-api.*"}
                  /
                  container_spec_memory_limit_bytes{pod=~"github-gists-api.*"}
                ) > 0.9
              instant: true
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "Memory usage is {{ humanizePercentage $value }} (threshold: 90%)"
          summary: "GitHub Gists API pod using too much memory"
        labels:
          severity: warning
          team: backend

      # CPU Throttling
      - uid: gist_api_cpu_throttle
        title: CPU Throttling Detected
        condition: A
        data:
          - refId: A
            queryType: prometheus
            model:
              expr: |
                rate(container_cpu_cfs_throttled_seconds_total{pod=~"github-gists-api.*"}[5m]) > 0.5
              instant: true
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "CPU throttling rate: {{ humanizePercentage $value }}"
          summary: "GitHub Gists API pod is being CPU throttled"
        labels:
          severity: info
          team: devops

# Notification channels (configure in Grafana UI or via API)
# Example: Slack, PagerDuty, Email, Webhook
contactPoints:
  - name: slack-alerts
    type: slack
    settings:
      url: ${SLACK_WEBHOOK_URL}
      text: |
        {{ range .Alerts }}
        *Alert:* {{ .Labels.alertname }}
        *Status:* {{ .Status }}
        *Summary:* {{ .Annotations.summary }}
        *Description:* {{ .Annotations.description }}
        {{ end }}

  - name: pagerduty-critical
    type: pagerduty
    settings:
      integrationKey: ${PAGERDUTY_INTEGRATION_KEY}
      severity: critical

# Notification policies
notificationPolicies:
  - receiver: slack-alerts
    group_by: ['alertname', 'severity']
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 4h
    matchers:
      - severity =~ warning|info
  
  - receiver: pagerduty-critical
    group_by: ['alertname']
    group_wait: 10s
    group_interval: 2m
    repeat_interval: 5m
    matchers:
      - severity = critical
      - page = "true"
